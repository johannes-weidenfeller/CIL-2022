## 21.06.2022 / Michael

- **Notebook**: The notebook `finetuning_bertweet.ipynb` allows finetuning BERTweet. It uses the [simpletransformers library](https://simpletransformers.ai/) (a wrapper around HuggingFace's [transformers library](https://github.com/huggingface/transformers)). Without yet having undergone any preprocessing, hyperparameter tuning or any other optimization, the current setup gives a training accuracy of `93.23`% and a testing accuracy of `90.76`%. It uses pretrained BERTweet-base (`135 M` parameters), a subset of the data (`200'000` tweets, of which `70%` were used for training and `30%` for testing) and the apparently SoTA parameters referred to in [this paper](https://openreview.net/pdf?id=nzpLWnVAyah) for finetuning (except for using only `1` epoch as opposed to `3`). Note that the parameters / code thatwas used for generating the submission currently ranking 3rd on the [kaggle leaderboard](https://www.kaggle.com/competitions/cil-text-classification-2022/leaderboard) is slightly different and was strongly inspired by [this kaggle notebook](https://www.kaggle.com/code/giovanni11/finetuning-bertweet-classification-score-85/notebook).
- **Next steps**: There are a wide range of things that I except to boost the performance and intent to focus on next, correspondingly, most importantly:
  - Finetuning BERTweet-large as opposed to BERTweet-base (with `355 M` parameters - more than `2.5x` as many).
  - Training on the full dataset as opposed to only a subset (with `2.5 M` tweets -  more than `12x` as many), i.e using `train_..._full.csv` as opposed to `train_....csv`.
  - Preprocessing tweets (e.g. removal of duplicate tweets, handling of special characters and tokens, etc.).
  - Hyperparameter tuning (e.g. with the help of [wandb](https://wandb.ai/), which allows for easy tracking of runs, plotting of loss curves, etc. and is supported by the transformers library).
  - Early stopping on moving average of evaluation loss.
  - Ensembling using multiple different and themselves strong models.
  - Trying different pretrained models.
  - Manual investigation to see whether there are any patterns in the mispredicted tweets (I imagine this might be useful especially with respect to preprocessing).
- **Creativity ideas**: As the creativity of the solution also determines part of the grade, we have to come up with something somewhat novel / non-obvious. I assume that it is unlikely that we get much performance boost from being very creative, thus I guess we could also focus on something that is not influencing the performance (directly). Some ideas are:
  - As opposed tos trying to improve the accuracy by say `1%` by using `10x` compute, we could try to reduce compute (i.e. training cost) significantly while only sacrificing marginal accuracy decrease.
  - Try to (build and) train a transformer from scratch instead of using a existing / pretrained model.
  - Try to achieve a reasonable (>> `80%`) accuracy by using "hardcoded" / very fast approach (e.g. significantly modifying / improving one of the classical approaches introduced in the lecture) for one of the baselines. 
  - Do very extensive and careful preprocessing.
  - Do some analysis on the labeling approach (e.g.: What accuracies do humans get on this task?, What kind of mistakes are there in the labels? What is the influence of this simplifying binary positive / negative sentiment assumption (i.e. tweets could be both / neither)?, etc.)
- **Paper Summary**: This is a summary (i.e. patchwork of fragments) of the already mentioned paper [On the Stability of Fine-tuning BERT](https://openreview.net/pdf?id=nzpLWnVAyah):
  *Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. Authors show that both hypotheses fail to explain the fine-tuning instability. Observed instability is caused by optimization difficulties that lead to vanishing gradients. Authors show that the observed fine-tuning instability can be decomposed into two separate aspects: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization late in training characterized by a large variance of development set accuracy for runs with almost equivalent training loss. Unless mentioned otherwise, authors follow the default fine-tuning strategy recommended by Devlin et al. (2019): fine-tuning uncased BERTLARGE (henceforth BERT) using a batch size of 16 and a learning rate of 2e−5. The learning rate is linearly increased from 0 to 2e−5 for the first 10% of iterations—which is known as a warmup—and linearly decreased to 0 afterward. They apply dropout with probability p = 0.1 and weight decay with λ = 0.01. They train for 3 epochs on all datasets and use global gradient clipping. Following Devlin et al. (2019), they use the AdamW optimizer
  (Loshchilov and Hutter, 2019) without bias correction. Code to reproduce results is based on HuggingFace's transformers library and is [available online](https://github.com/uds-lsv/bert-stable-fine-tuning)*.

## 22.06.2022 / Michael
- **Baseline Model**: Added a simple (i.e. classical / hardcoded) baseline model that computes sentiment-wise occurrence counters of `n`-tuples. Sentiment-wise `n`-tuple scores are simply occurrence counts scaled by a power `p`. For a given tweet, a score is built for each sentiment as the sum of scores for each consecutive n-tuple of words in the tweet. Then the probability for the positive label is simply the share of positive score to overall score (unless overall score is zero). Reaches a training accuracy of `88.13%` and a test accuracy of `80.04%` (slightly below the grade 4 baseline) using again the small training set with a 70-30 split and `n=2` (i.e. using word pairs) and `p=0.25` (determined by crude optimization / eyeballing). Could surely be extended and adjusted such as to get a better performance. See `finetuning_bertweet.ipynb`.

## 03.07.2022 / Michael

- **Sensitivity Analysis**: The goal here was to investigate the sensitivity of the performance (i.e. accuracy) in changing certain hyperparameters or the like. Due to runtime reasons, I restricted the analysis to an even smaller dataset, namely 10'000 samples. With the same hyperparameters as used for the run on the subset of 200'000 tweets, this even greater reduction of the training data size only resulted in a performance decrease of about 2%, i.e. from an accuracy of 90.76% to 88.51% on the test set. For each of the size of the training data, batch size, learning rate and momentum (most relevant non-library-default parameters besides number of epochs), there are four runs, once with a quarter, once with half, once with double and once with quadruple the "default" (see default_config) values, with otherwise unchanged settings for allowing a better isolated view. For all except the training data size, these parameters seem to be at somewhat optimal values already, with respect to the test set accuracy, at least for this number of training samples. Increasing the training data size seems to give a rather consistent roughly log-order improvement of the accuracy. Surely, it would be interesting to those sensitivities potentially change for larger training data, as well as to look at sensitivities towards other things such as number of models in ensembles, number of epochs, etc.

- **Preprocessing**: Here I simply applied the preprocessing provided by VinAI used for BERTweet, to see whether this improves performance. The code is taken from [here](https://github.com/VinAIResearch/BERTweet/blob/master/TweetNormalizer.py). This does not seem to be the case, as test set accuracy decreased slightly from 88.51% to 88.24% on the test set (and, more significantly, train set accuracy decreased from 91.97% to 90.50%). Potentially, preprocessing is not that crucial after all, however this may also be too early to conclude.

- **Ensembling**: The main hypothesis here is that different models (with similar and good performance) probably also have different kinds of weaknesses, which to some degree could potentially be compensated via ensembling. Currently, the simplest form was used: Only three models were trained, and only the order of the training data presented to the models was changed, and the ensemble predictions are simply determined by the mode of the individual model predictions. This did increase performance somewhat, i.e. from 88.51% to 88.78% on the test set, for a training data size of 10'000. Obviously, this effect may be greater for more models, larger training data and more sophisticated ensembling approach. Generally, ensembling could happen in at least one of the following three ways:
  - majorty vote on predicted labels (requires odd number of models)
  - infer label from (geometric) mean of predicted probabilities
  - if one model dominates w.r.t. performance (i.e. serves as the golden standard), one could e.g. only consider cases where the predicted probabilities by the golden standard model fall within a certain "uncertainty" region, e.g. in range [0.4, 0.6], where one could then use other model(s) as fallbacks for such corner cases, i.e. in that they potentially nudge the probabilities of low-confidence tweets (as seen by the gold standard model) in the right direction.
    The things that vary across the models in the ensemble could be any of the following:
  - the model or model architecture
  - the model hyperparameters
  - training data subsets / orders
- **Finetuning**: It might be worthwile to play around with varying the finetuning approach, e.g. only considering subsets of the model parameters (i.e. only the last layer(s)), using model heads (i.e. using additional, not yet trained parameters on top of BERTweet), etc., however I expect this to be less straight-forward (i.e. a bit more involved) and less rewarding (i.e. not giving much better accuracy) as other things one could focus on instead.

## 10.07.2022 / Michael
- **Experiments Summary** Here's a summary of the experiments ran on 10k examples (see other notes above / below for details):
- |index|ensembling|benchmark|smileys|curriculum|normalization|unique|tensorflow|
  |---|---|---|---|---|---|---|---|
  |train|92\.25%|92\.19%|91\.80%|91\.44%|90\.50%|91\.70%|92\.79%|
  |test|88\.78%|88\.61%|88\.51%|88\.37%|88\.24%|87\.93%|87\.78%|
- **Euler**: The goal was to make the code run on ETH's [Euler](https://scicomp.ethz.ch/wiki/Euler) and make use of GPUs for smaller runtimes. After running into issues with package versions and dependencies, I also wrote a class to finetune BertWeet using pure tensorflow avoiding the use of simpletransformers. This I one did manage to make run on Euler, however not on the GPU - again, due to issues with dependencies (libhdf5 / libcudart). Currently, the (pure) tensorflow implementation `TFBertweetClassifier` performs slightly worse than the simpletransformers implementation `BertweetClassifier`, as I haven't yet integrated AdamW (i.e. `87.78%` vs. `88.78%` test accuracy on a model trained on 10k examples). While I managed to make it run with AdamW, it did not perform good at all, i.e. about as good as random guessing, thus I am most likely missing some other adjustment here. If anyone feels like trying to go ahead with this (I still believe this implementation might be useful down the road, when we want to be more flexible w.r.t. e.g. optimizing, learning rate scheduling, etc.), see `tfa.optimizers.AdamW`. It seems that the model using the large dataset trains fairly quickly on Lucas' own GPU, thus I think it is not crucial to make it work on Euler. However, if anyone feels like trying, see **euler.py**. To run on Euler, you can get started by running these commands:
  - `ssh username@euler.ethz.ch`
  - `module load gcc/6.3.0 python_gpu/3.7.4`
  - `bsub -n 1 -W 4:00 -R "rusage[mem=2048, ngpus_excl_p=1]" python euler.py`
- **Curriculum learning**: The idea here was that a random order of data presented to the model during training is unlikely to be the best order, and that curriculum learning, that is, training from easier to harder data, could (i.e. should be able to) improve the model performance. To build a curriculum, I first trained a model and then used its confidence in its predictions, i.e. the maximum predicted class probability, as a proxy score for the "easiness" of the example, and then sorted the training examples by decreasing scores. One issue here was that the confidence scores are biased in the class, that is, in the first run, this led to having the easiest (i.e. first) roughly 20% of the data almost all belonging to only one class, and many of the hardest examples belonging to the other class. This was deemed infeasible, thus this was worked around by interleaving the examples such that the within-class order easiness is preserved, but the two class frequencies in any interval are roughly equal (see `get_confidence_scores` and `build_curriculum` in `helpers.py`). Currently, this approach does not give any improvements, i.e. gives a `88.37%` test accuracy as opposed to a `88.78%` benchmark accuracy (10k examples). I am not writing this off as of yet however, as the approach is still flawed, in the sense that it does not yet account for correctness, i.e. only looks at the probabilities, as well as there is also other room for improvement, e.g. sort only subsets of the data, etc.
- **Optimization**: I set up a simple framwork for optimizing hyperparameters, see `optimize` in `helpers.py`. The idea is to do a guided blackbox optimization over a set of hyperparameters (by providing either ranges for continuous parameters or sets for discrete ones) using hyperopt and a select number of runs. However, no optimization was performed as of yet.
- **Smiley Reverse Engineering**: The assumption here is that the tweets are labeles based on occurrences of ":)" and ":(" and processed by simply removing ":)" and ":(". For cases where smileys ":))" or ":((" were used, this would leave extra ")" or "(" in as artifacts in the tweets. I tried identifiying all those tweets to which this applies (see `get_smileys_subset`) by searching for occurrences of ')' without '(' (or vice versa) and watching out not conflating the tweets i try to detect with tweets with other artifacts such as tags, retweets, etc. This approach identifies about `16%` of the data, on which, if simply predicting based on the heuristic *if ':))' believed to occur in tweet: predict positive, if ':((' believed to occur in tweet: predict negative*. This gives an accuracy of `98.28%` on this `16%` of the data. However, when trying to make use of this by training a model and then overruling its predictions wherever the identification rule indicates based on this simple heuristic, the model performance did not improve, it actually worsened a bit. As it turns out, only a small minority (`<<16%`) of the predictions were changed by this update rule and the model already cathes on on exactly these tweets very well, with model accuracies higher than the `98.28%` on them. Thus this turned out to not be of any use.
- **Miscellaneous**. There are many possibilities for next steps which I think could pay off:
  - make the code run on Euler, as significantly faster runtimes would enable much more extensive experimentation.
  - make AdamW work with the TF implementation.
  - re-run experiments and sensitivity analysis using larger training sizes, to be able to make more confident conclusions.
  - pick a handful of hyperparameters deemed to be of great importance and see what optimization with hyperopt brings.
  - improve on curriculum learning approach.

### 19.07.2022 / Michael
- **Refactoring**: Code now running on euler, refactoring and trying to write somewhat more clean code.
- **Experiments**: Ran various experiments, grouped into:
  - Preprocessing: no duplicates, smiley reconstruction, vinai preprocessing
  - Subset: determine subsets to either use exclusively or augment training data with, e.g. train with the hardest/easiest x% duplicated, train with only hard/easy examples, etc.
  - Curriculum: order training data by hardness measures, w/ or /wo some reordering due to class imbalances & too strict of ordering
  - Ensembling: various inference techniques / variance introduction methods (i.e. changing order of training data and weight initialization across models in ensemble)
  - Hyperparameter: some plays with smaller/larger batch size/learning rate
 
  Experiments were all run on the same `25k` training samples (`1%` of the data). See `euler/full_results_1_pct.csv` for results (sorted by decresing `test_acc`).
- **Exploratory Data Analysis**: Mainly looking at frequencies of certain properties between classes and then ranking them by class bias, e.g.
  - most common tweets
  - tweet lengths distribution
  - most common tokens
  - hashtag analysis
  - special token overview
  - single-character tokens
 
  See `euler/full_eda_res.txt` for overview.
- **New Baseline**: Using gradient boosting on top of embeddings from transformers via `sentence-transformers` and `catboost`. Not yet thoroughly explored, but around `2%` worse w.r.t test set accuracy than the default finetuned Bertweet on around `10k` examples.
- **Next Up Ideas**:
  - Validate results (i.e. reduce chance of observing varying performances due to chance) either by re-running on different data / with different seed / more data
  - Combine / expand on / add to the promising experiments and run on larger training sets (e.g. `10%`)
  - Re-Iterate