## 21.06.2022 / Michael

- The code that was used for generating the submission currently ranking 3rd on the [kaggle leaderboard](https://www.kaggle.com/competitions/cil-text-classification-2022/leaderboard) was strongly inspired by [this kaggle notebook](https://www.kaggle.com/code/giovanni11/finetuning-bertweet-classification-score-85/notebook).
- The notebook `finetuning_bertweet.ipynb` allows finetuning BERTweet. It uses the [simpletransformers library](https://simpletransformers.ai/) (a wrapper around HuggingFace's [transformers library](https://github.com/huggingface/transformers)).
- Without yet having undergone any preprocessing, hyperparameter tuning or any other optimization, the current setup gives a training accuracy of `93.23`% and a testing accuracy of `90.76`%. It uses pretrained BERTweet-base (`135 M` parameters), a subset of the data (`200'000` tweets, of which `70%` were used for training and `30%` for testing) and the apparently SoTA parameters referred to in [this paper](https://openreview.net/pdf?id=nzpLWnVAyah) for finetuning (except for using only `1` epoch as opposed to `3`).
- There are a wide range of things that I except to boost the performance and intent to focus on next, correspondingly, most importantly:
  - Finetuning BERTweet-large as opposed to BERTweet-base (with `355 M` parameters - more than `2.5x` as many).
  - Training on the full dataset as opposed to only a subset (with `2.5 M` tweets -  more than `12x` as many), i.e using `train_..._full.csv` as opposed to `train_....csv`.
  - Preprocessing tweets (e.g. removal of duplicate tweets, handling of special characters and tokens, etc.).
  - Hyperparameter tuning (e.g. with the help of [wandb](https://wandb.ai/), which allows for easy tracking of runs, plotting of loss curves, etc. and is supported by the transformers library).
  - Early stopping on moving average of evaluation loss.
  - Ensembling using multiple different and themselves strong models.
  - Trying different pretrained models.
  - Manual investigation to see whether there are any patterns in the mispredicted tweets (I imagine this might be useful especially with respect to preprocessing).
- As the creativity of the solution also determines part of the grade, we have to come up with something somewhat novel / non-obvious. I assume that it is unlikely that we get much performance boost from being very creative, thus I guess we could also focus on something that is not influencing the performance (directly). Some ideas are:
  - As opposed tos trying to improve the accuracy by say `1%` by using `10x` compute, we could try to reduce compute (i.e. training cost) significantly while only sacrificing marginal accuracy decrease.
  - Try to (build and) train a transformer from scratch instead of using a existing / pretrained model.
  - Try to achieve a reasonable (>> `80%`) accuracy by using "hardcoded" / very fast approach (e.g. significantly modifying / improving one of the classical approaches introduced in the lecture) for one of the baselines. 
  - Do very extensive and careful preprocessing.
  - Do some analysis on the labeling approach (e.g.: What accuracies do humans get on this task?, What kind of mistakes are there in the labels? What is the influence of this simplifying binary positive / negative sentiment assumption (i.e. tweets could be both / neither)?, etc.)
- This is a summary (i.e. patchwork of fragments) of the already mentioned paper [On the Stability of Fine-tuning BERT](https://openreview.net/pdf?id=nzpLWnVAyah):
  *Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. Authors show that both hypotheses fail to explain the fine-tuning instability. Observed instability is caused by optimization difficulties that lead to vanishing gradients. Authors show that the observed fine-tuning instability can be decomposed into two separate aspects: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization late in training characterized by a large variance of development set accuracy for runs with almost equivalent training loss. Unless mentioned otherwise, authors follow the default fine-tuning strategy recommended by Devlin et al. (2019): fine-tuning uncased BERTLARGE (henceforth BERT) using a batch size of 16 and a learning rate of 2e−5. The learning rate is linearly increased from 0 to 2e−5 for the first 10% of iterations—which is known as a warmup—and linearly decreased to 0 afterward. They apply dropout with probability p = 0.1 and weight decay with λ = 0.01. They train for 3 epochs on all datasets and use global gradient clipping. Following Devlin et al. (2019), they use the AdamW optimizer
  (Loshchilov and Hutter, 2019) without bias correction. Code to reproduce results is based on HuggingFace's transformers library and is [available online](https://github.com/uds-lsv/bert-stable-fine-tuning)*.
 