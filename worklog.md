## 21.06.2022 / Michael

- **Notebook**: The notebook `finetuning_bertweet.ipynb` allows finetuning BERTweet. It uses the [simpletransformers library](https://simpletransformers.ai/) (a wrapper around HuggingFace's [transformers library](https://github.com/huggingface/transformers)). Without yet having undergone any preprocessing, hyperparameter tuning or any other optimization, the current setup gives a training accuracy of `93.23`% and a testing accuracy of `90.76`%. It uses pretrained BERTweet-base (`135 M` parameters), a subset of the data (`200'000` tweets, of which `70%` were used for training and `30%` for testing) and the apparently SoTA parameters referred to in [this paper](https://openreview.net/pdf?id=nzpLWnVAyah) for finetuning (except for using only `1` epoch as opposed to `3`). Note that the parameters / code thatwas used for generating the submission currently ranking 3rd on the [kaggle leaderboard](https://www.kaggle.com/competitions/cil-text-classification-2022/leaderboard) is slightly different and was strongly inspired by [this kaggle notebook](https://www.kaggle.com/code/giovanni11/finetuning-bertweet-classification-score-85/notebook).
- **Next steps**: There are a wide range of things that I except to boost the performance and intent to focus on next, correspondingly, most importantly:
  - Finetuning BERTweet-large as opposed to BERTweet-base (with `355 M` parameters - more than `2.5x` as many).
  - Training on the full dataset as opposed to only a subset (with `2.5 M` tweets -  more than `12x` as many), i.e using `train_..._full.csv` as opposed to `train_....csv`.
  - Preprocessing tweets (e.g. removal of duplicate tweets, handling of special characters and tokens, etc.).
  - Hyperparameter tuning (e.g. with the help of [wandb](https://wandb.ai/), which allows for easy tracking of runs, plotting of loss curves, etc. and is supported by the transformers library).
  - Early stopping on moving average of evaluation loss.
  - Ensembling using multiple different and themselves strong models.
  - Trying different pretrained models.
  - Manual investigation to see whether there are any patterns in the mispredicted tweets (I imagine this might be useful especially with respect to preprocessing).
- **Creativity ideas**: As the creativity of the solution also determines part of the grade, we have to come up with something somewhat novel / non-obvious. I assume that it is unlikely that we get much performance boost from being very creative, thus I guess we could also focus on something that is not influencing the performance (directly). Some ideas are:
  - As opposed tos trying to improve the accuracy by say `1%` by using `10x` compute, we could try to reduce compute (i.e. training cost) significantly while only sacrificing marginal accuracy decrease.
  - Try to (build and) train a transformer from scratch instead of using a existing / pretrained model.
  - Try to achieve a reasonable (>> `80%`) accuracy by using "hardcoded" / very fast approach (e.g. significantly modifying / improving one of the classical approaches introduced in the lecture) for one of the baselines. 
  - Do very extensive and careful preprocessing.
  - Do some analysis on the labeling approach (e.g.: What accuracies do humans get on this task?, What kind of mistakes are there in the labels? What is the influence of this simplifying binary positive / negative sentiment assumption (i.e. tweets could be both / neither)?, etc.)
- **Paper Summary**: This is a summary (i.e. patchwork of fragments) of the already mentioned paper [On the Stability of Fine-tuning BERT](https://openreview.net/pdf?id=nzpLWnVAyah):
  *Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. Authors show that both hypotheses fail to explain the fine-tuning instability. Observed instability is caused by optimization difficulties that lead to vanishing gradients. Authors show that the observed fine-tuning instability can be decomposed into two separate aspects: (1) optimization difficulties early in training, characterized by vanishing gradients, and (2) differences in generalization late in training characterized by a large variance of development set accuracy for runs with almost equivalent training loss. Unless mentioned otherwise, authors follow the default fine-tuning strategy recommended by Devlin et al. (2019): fine-tuning uncased BERTLARGE (henceforth BERT) using a batch size of 16 and a learning rate of 2e−5. The learning rate is linearly increased from 0 to 2e−5 for the first 10% of iterations—which is known as a warmup—and linearly decreased to 0 afterward. They apply dropout with probability p = 0.1 and weight decay with λ = 0.01. They train for 3 epochs on all datasets and use global gradient clipping. Following Devlin et al. (2019), they use the AdamW optimizer
  (Loshchilov and Hutter, 2019) without bias correction. Code to reproduce results is based on HuggingFace's transformers library and is [available online](https://github.com/uds-lsv/bert-stable-fine-tuning)*.

## 22.06.2022 / Michael
- **Baseline Model**: Added a simple (i.e. classical / hardcoded) baseline model that computes sentiment-wise occurrence counters of `n`-tuples. Sentiment-wise `n`-tuple scores are simply occurrence counts scaled by a power `p`. For a given tweet, a score is built for each sentiment as the sum of scores for each consecutive n-tuple of words in the tweet. Then the probability for the positive label is simply the share of positive score to overall score (unless overall score is zero). Reaches a training accuracy of `88.13%` and a test accuracy of `80.04%` (slightly below the grade 4 baseline) using again the small training set with a 70-30 split and `n=2` (i.e. using word pairs) and `p=0.25` (determined by crude optimization / eyeballing). Could surely be extended and adjusted such as to get a better performance. See `finetuning_bertweet.ipynb`.

## 03.07.2022 / Michael

- **Sensitivity Analysis**: The goal here was to investigate the sensitivity of the performance (i.e. accuracy) in changing certain hyperparameters or the like. Due to runtime reasons, I restricted the analysis to an even smaller dataset, namely 10'000 samples. With the same hyperparameters as used for the run on the subset of 200'000 tweets, this even greater reduction of the training data size only resulted in a performance decrease of about 2%, i.e. from an accuracy of 90.76% to 88.51% on the test set. For each of the size of the training data, batch size, learning rate and momentum (most relevant non-library-default parameters besides number of epochs), there are four runs, once with a quarter, once with half, once with double and once with quadruple the "default" (see default_config) values, with otherwise unchanged settings for allowing a better isolated view. For all except the training data size, these parameters seem to be at somewhat optimal values already, with respect to the test set accuracy, at least for this number of training samples. Increasing the training data size seems to give a rather consistent roughly log-order improvement of the accuracy. Surely, it would be interesting to those sensitivities potentially change for larger training data, as well as to look at sensitivities towards other things such as number of models in ensembles, number of epochs, etc.

- **Preprocessing**: Here I simply applied the preprocessing provided by VinAI used for BERTweet, to see whether this improves performance. The code is taken from [here](https://github.com/VinAIResearch/BERTweet/blob/master/TweetNormalizer.py). This does not seem to be the case, as test set accuracy decreased slightly from 88.51% to 88.24% on the test set (and, more significantly, train set accuracy decreased from 91.97% to 90.50%). Potentially, preprocessing is not that crucial after all, however this may also be too early to conclude.

- **Ensembling**: The main hypothesis here is that different models (with similar and good performance) probably also have different kinds of weaknesses, which to some degree could potentially be compensated via ensembling. Currently, the simplest form was used: Only three models were trained, and only the order of the training data presented to the models was changed, and the ensemble predictions are simply determined by the mode of the individual model predictions. This did increase performance somewhat, i.e. from 88.51% to 88.78% on the test set, for a training data size of 10'000. Obviously, this effect may be greater for more models, larger training data and more sophisticated ensembling approach. Generally, ensembling could happen in at least one of the following three ways:
  - majorty vote on predicted labels (requires odd number of models)
  - infer label from (geometric) mean of predicted probabilities
  - if one model dominates w.r.t. performance (i.e. serves as the golden standard), one could e.g. only consider cases where the predicted probabilities by the golden standard model fall within a certain "uncertainty" region, e.g. in range [0.4, 0.6], where one could then use other model(s) as fallbacks for such corner cases, i.e. in that they potentially nudge the probabilities of low-confidence tweets (as seen by the gold standard model) in the right direction.
    The things that vary across the models in the ensemble could be any of the following:
  - the model or model architecture
  - the model hyperparameters
  - training data subsets / orders
- **Finetuning**: It might be worthwile to play around with varying the finetuning approach, e.g. only considering subsets of the model parameters (i.e. only the last layer(s)), using model heads (i.e. using additional, not yet trained parameters on top of BERTweet), etc., however I expect this to be less straight-forward (i.e. a bit more involved) and less rewarding (i.e. not giving much better accuracy) as other things one could focus on instead.